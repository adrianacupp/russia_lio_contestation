{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import csv\n",
    "import math\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.utils import resample\n",
    "import random\n",
    "\n",
    "random.seed(3919)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.test.utils \n",
    "import tqdm\n",
    "import itertools\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('/Users/adrianacuppuleri/Desktop/GITHUB ADRIANA/Illiberal_discourse/data/corpus_adriana/corpus_president_of_russia/merged_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: 8265302\n"
     ]
    }
   ],
   "source": [
    "all_text = \" \".join(df['text_clean'])\n",
    "word_count = len(re.findall(r'\\w+', all_text))\n",
    "print(\"Total number of words:\", word_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adjust compund words that will be used later for the codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'миропорядок' is present in at least one row.\n",
      "The word appears in 121 rows.\n"
     ]
    }
   ],
   "source": [
    "word_to_find = 'миропорядок'\n",
    "mask = df['text_clean'].str.contains(word_to_find)\n",
    "\n",
    "if mask.any():\n",
    "    print(f\"The word '{word_to_find}' is present in at least one row.\")\n",
    "    rows_with_word = df[mask]\n",
    "    print(f\"The word appears in {len(rows_with_word)} rows.\")\n",
    "else:\n",
    "    print(f\"The word '{word_to_find}' is not present in any row.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'] = df['text_clean'].apply(lambda x: x.replace(\"мировой порядок\", \"миропорядок\") \n",
    "                if any(phrase in x for phrase in [\"мировой порядок\", \"мирового порядка\", \n",
    "                                                  \"мировому порядку\", \"мировым порядком\",\n",
    "                                                  \"мировом порядке\"]) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'миропорядок' is present in at least one row.\n",
      "The word appears in 131 rows.\n"
     ]
    }
   ],
   "source": [
    "word_to_find = 'миропорядок'\n",
    "mask = df['text_clean'].str.contains(word_to_find)\n",
    "\n",
    "if mask.any():\n",
    "    print(f\"The word '{word_to_find}' is present in at least one row.\")\n",
    "    rows_with_word = df[mask]\n",
    "    print(f\"The word appears in {len(rows_with_word)} rows.\")\n",
    "else:\n",
    "    print(f\"The word '{word_to_find}' is not present in any row.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'русский мир' is present in at least one row.\n",
      "The word appears in 65 rows.\n"
     ]
    }
   ],
   "source": [
    "word_to_find = 'русский мир'\n",
    "mask = df['text_clean'].str.contains(word_to_find)\n",
    "\n",
    "if mask.any():\n",
    "    print(f\"The word '{word_to_find}' is present in at least one row.\")\n",
    "    rows_with_word = df[mask]\n",
    "    print(f\"The word appears in {len(rows_with_word)} rows.\")\n",
    "else:\n",
    "    print(f\"The word '{word_to_find}' is not present in any row.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform all declension of русский мир in compound word with _\n",
    "# nom: русский мир\n",
    "# gen: русского мира\n",
    "# dat: русскому миру\n",
    "# acc: русский мир\n",
    "# instr: русским миром\n",
    "# prepos: русском мире"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if русский мир (in all its declension) is present then русский_мир\n",
    "df['text_clean'] = df['text_clean'].apply(lambda x: x.replace(\"русский мир\", \"русский_мир\"))\n",
    "df['text_clean'] = df['text_clean'].apply(lambda x: x.replace(\"русского мира\", \"русский_мир\"))\n",
    "df['text_clean'] = df['text_clean'].apply(lambda x: x.replace(\"русскому миру\", \"русский_мир\"))\n",
    "df['text_clean'] = df['text_clean'].apply(lambda x: x.replace(\"русским миром\", \"русский_мир\"))\n",
    "df['text_clean'] = df['text_clean'].apply(lambda x: x.replace(\"русском мире\", \"русский_мир\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'русский_мир' is present in at least one row.\n",
      "The word appears in 130 rows.\n"
     ]
    }
   ],
   "source": [
    "word_to_find = 'русский_мир'\n",
    "mask = df['text_clean'].str.contains(word_to_find)\n",
    "\n",
    "if mask.any():\n",
    "    print(f\"The word '{word_to_find}' is present in at least one row.\")\n",
    "    rows_with_word = df[mask]\n",
    "    print(f\"The word appears in {len(rows_with_word)} rows.\")\n",
    "else:\n",
    "    print(f\"The word '{word_to_find}' is not present in any row.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'национальная безопасность' is present in at least one row.\n",
      "The word appears in 10 rows.\n"
     ]
    }
   ],
   "source": [
    "word_to_find = 'национальная безопасность'\n",
    "mask = df['text_clean'].str.contains(word_to_find)\n",
    "\n",
    "if mask.any():\n",
    "    print(f\"The word '{word_to_find}' is present in at least one row.\")\n",
    "    rows_with_word = df[mask]\n",
    "    print(f\"The word appears in {len(rows_with_word)} rows.\")\n",
    "else:\n",
    "    print(f\"The word '{word_to_find}' is not present in any row.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same for национальная_безопасность\n",
    "df['text_clean'] = df['text_clean'].apply(lambda x: x.replace(\"национальная безопасность\", \"национальная_безопасность\"))\n",
    "df['text_clean'] = df['text_clean'].apply(lambda x: x.replace(\"национальной безопасности\", \"национальная_безопасность\"))\n",
    "df['text_clean'] = df['text_clean'].apply(lambda x: x.replace(\"национальную безопасность\", \"национальная_безопасность\"))\n",
    "df['text_clean'] = df['text_clean'].apply(lambda x: x.replace(\"национальной безопасностью\", \"национальная_безопасность\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'национальная_безопасность' is present in at least one row.\n",
      "The word appears in 246 rows.\n"
     ]
    }
   ],
   "source": [
    "word_to_find = 'национальная_безопасность'\n",
    "mask = df['text_clean'].str.contains(word_to_find)\n",
    "\n",
    "if mask.any():\n",
    "    print(f\"The word '{word_to_find}' is present in at least one row.\")\n",
    "    rows_with_word = df[mask]\n",
    "    print(f\"The word appears in {len(rows_with_word)} rows.\")\n",
    "else:\n",
    "    print(f\"The word '{word_to_find}' is not present in any row.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'международное право' is present in at least one row.\n",
      "The word appears in 414 rows.\n"
     ]
    }
   ],
   "source": [
    "# same for международное право\n",
    "word_to_find = 'международное право'\n",
    "mask = df['text_clean'].str.contains(word_to_find)\n",
    "\n",
    "if mask.any():\n",
    "    print(f\"The word '{word_to_find}' is present in at least one row.\")\n",
    "    rows_with_word = df[mask]\n",
    "    print(f\"The word appears in {len(rows_with_word)} rows.\")\n",
    "else:\n",
    "    print(f\"The word '{word_to_find}' is not present in any row.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'] = df['text_clean'].apply(lambda x: x.replace(\"международное право\", \"международное_право\"))\n",
    "df['text_clean'] = df['text_clean'].apply(lambda x: x.replace(\"международного права\", \"международное_право\"))\n",
    "df['text_clean'] = df['text_clean'].apply(lambda x: x.replace(\"международному праву\", \"международное_право\"))\n",
    "df['text_clean'] = df['text_clean'].apply(lambda x: x.replace(\"международным правом\", \"международное_право\"))\n",
    "df['text_clean'] = df['text_clean'].apply(lambda x: x.replace(\"международном праве\", \"международное_право\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'международное_право' is present in at least one row.\n",
      "The word appears in 1554 rows.\n"
     ]
    }
   ],
   "source": [
    "word_to_find = 'международное_право'\n",
    "mask = df['text_clean'].str.contains(word_to_find)\n",
    "\n",
    "if mask.any():\n",
    "    print(f\"The word '{word_to_find}' is present in at least one row.\")\n",
    "    rows_with_word = df[mask]\n",
    "    print(f\"The word appears in {len(rows_with_word)} rows.\")\n",
    "else:\n",
    "    print(f\"The word '{word_to_find}' is not present in any row.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chunk the df into 4 time frames according to presidential mandate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2003-10-06\n",
       "1      2003-11-11\n",
       "2      2003-11-13\n",
       "3      2003-12-03\n",
       "4      2004-01-12\n",
       "          ...    \n",
       "689    2022-12-21\n",
       "688    2022-12-21\n",
       "113    2023-01-13\n",
       "690    2023-02-21\n",
       "11     2023-02-21\n",
       "Name: date, Length: 6779, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trasnform date into datetime\n",
    "df['date'] = pd.to_datetime(df.date)\n",
    "df['date'] = df['date'].dt.strftime('%Y-%m-%d')\n",
    "df['date'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create boolean masks for each date range\n",
    "mask1 = (df['date'] >= '2003-01-01') & (df['date'] <= '2008-03-02')\n",
    "mask2 = (df['date'] >= '2008-03-03') & (df['date'] <= '2012-03-04')\n",
    "mask3 = (df['date'] >= '2012-03-05') & (df['date'] <= '2018-03-18')\n",
    "mask4 = (df['date'] >= '2018-03-19') & (df['date'] <= '2023-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create smaller dataframes using boolean indexing\n",
    "t1 = df.loc[mask1] # second Putin's mandate (plus_ a couple of doc from 2003 in Lavrov's):2004-2008\n",
    "t2 = df.loc[mask2] # Medvedev: 2008-2012\n",
    "t3 = df.loc[mask3] # third Putin's mandate: 2012-2018\n",
    "t4 = df.loc[mask4] # fourth Putin's mandate: 2018 - till present (2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the four dataframes in separate pickle files\n",
    "t1.to_pickle('/Users/adrianacuppuleri/Desktop/GITHUB ADRIANA/Illiberal_discourse/data/corpus_adriana/corpus_president_of_russia/t1.pkl')\n",
    "t2.to_pickle('/Users/adrianacuppuleri/Desktop/GITHUB ADRIANA/Illiberal_discourse/data/corpus_adriana/corpus_president_of_russia/t2.pkl')\n",
    "t3.to_pickle('/Users/adrianacuppuleri/Desktop/GITHUB ADRIANA/Illiberal_discourse/data/corpus_adriana/corpus_president_of_russia/t3.pkl')\n",
    "t4.to_pickle('/Users/adrianacuppuleri/Desktop/GITHUB ADRIANA/Illiberal_discourse/data/corpus_adriana/corpus_president_of_russia/t4.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random sample of 10% for each era"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sample_t1 = t1.sample(frac=0.1, replace=False)\n",
    "sample_t2 = t2.sample(frac=0.1, replace=False)\n",
    "sample_t3 = t3.sample(frac=0.1, replace=False)\n",
    "sample_t4 = t4.sample(frac=0.1, replace=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use stem (words that starts as...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the file\n",
    "t1 = pd.read_pickle('/Users/adrianacuppuleri/Desktop/GITHUB ADRIANA/Illiberal_discourse/data/corpus_adriana/corpus_president_of_russia/t1.pkl')\n",
    "t2 = pd.read_pickle('/Users/adrianacuppuleri/Desktop/GITHUB ADRIANA/Illiberal_discourse/data/corpus_adriana/corpus_president_of_russia/t2.pkl')\n",
    "t3 = pd.read_pickle('/Users/adrianacuppuleri/Desktop/GITHUB ADRIANA/Illiberal_discourse/data/corpus_adriana/corpus_president_of_russia/t3.pkl')\n",
    "t4 = pd.read_pickle('/Users/adrianacuppuleri/Desktop/GITHUB ADRIANA/Illiberal_discourse/data/corpus_adriana/corpus_president_of_russia/t4.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "eurasianism = ['еврази','евразийск','русский_мир', 'цивилизаци', 'запад', 'нацисм']\n",
    "\n",
    "westphalianism = ['великодержавн','статус','суверенн','международное_право']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "security_concerns = ['сфер', 'национальная_безопасность','противовес','дестабилиз','нато','угроз']\n",
    "\n",
    "multipolarism = ['многополярн', 'партнерств', 'сотрудничеств','альтернатив', 'выбор', 'развити']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform data in list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTIRE DATA\n",
    "#transform each timeframe df in a list\n",
    "era1 = t1['text_clean'].tolist()\n",
    "era2 = t2['text_clean'].tolist()\n",
    "era3 = t3['text_clean'].tolist()\n",
    "era4 = t4['text_clean'].tolist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 10% SAMPLE\n",
    "#transform each timeframe df in a list\n",
    "era1 = sample_t1['text_clean'].tolist()\n",
    "era2 = sample_t2['text_clean'].tolist()\n",
    "era3 = sample_t3['text_clean'].tolist()\n",
    "era4 = sample_t4['text_clean'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_lists = []\n",
    "list_of_lists.append(era1)\n",
    "list_of_lists.append(era2)\n",
    "list_of_lists.append(era3)\n",
    "list_of_lists.append(era4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('в', 25), ('и', 25), ('россии', 9), ('к', 9), ('мы', 9), ('это', 7), ('на', 6), ('что', 6), ('с', 6), ('страны', 5)]\n"
     ]
    }
   ],
   "source": [
    "word_freqs = [Counter(sent.split()) for sent in era4]\n",
    "# print the top 10 most common words in the first document\n",
    "print(word_freqs[0].most_common(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating over lists of sentences by era to do era-by-era word2vec modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrapping see rodman\n",
    "# model: sg\n",
    "# vector_size: 300\n",
    "# n_neighbours: 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for bootstrapping\n",
    "# used to generate new samples for training the Word2Vec model\n",
    "def bootstrap_eras(list_of_lists, n_bootstraps=50, vector_size=100, min_count=0, epochs=200, sg=1, hs=0, negative=5, window=15, workers=4):\n",
    "    random.seed(3919)\n",
    "    bootstrapped_eras = []\n",
    "    for era_idx, era in enumerate(list_of_lists):\n",
    "        sim_stats_era = []\n",
    "        for i in range(n_bootstraps):\n",
    "            sentence_samples = resample(era)\n",
    "            model = Word2Vec(sentence_samples, vector_size=vector_size, min_count=min_count, epochs=epochs, sg=sg, hs=hs, negative=negative, window=window, workers=workers)\n",
    "            sim_stats_era.append(model)\n",
    "            # Save each bootstrapped model as a .pkl file\n",
    "            filename = f\"models/bootstrap/era{era_idx+1}_bootstrap{i+1}.pkl\"\n",
    "            os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "            with open(filename, \"wb\") as f:\n",
    "                pickle.dump(model, f)\n",
    "            print(f\"Finished with run {i+1} out of {n_bootstraps} for era {era_idx+1}\")\n",
    "        bootstrapped_eras.append(sim_stats_era)\n",
    "    return bootstrapped_eras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished with run 1 out of 50 for era 1\n",
      "Finished with run 2 out of 50 for era 1\n",
      "Finished with run 3 out of 50 for era 1\n",
      "Finished with run 4 out of 50 for era 1\n",
      "Finished with run 5 out of 50 for era 1\n",
      "Finished with run 6 out of 50 for era 1\n",
      "Finished with run 7 out of 50 for era 1\n",
      "Finished with run 8 out of 50 for era 1\n",
      "Finished with run 9 out of 50 for era 1\n",
      "Finished with run 10 out of 50 for era 1\n",
      "Finished with run 11 out of 50 for era 1\n",
      "Finished with run 12 out of 50 for era 1\n",
      "Finished with run 13 out of 50 for era 1\n",
      "Finished with run 14 out of 50 for era 1\n",
      "Finished with run 15 out of 50 for era 1\n",
      "Finished with run 16 out of 50 for era 1\n",
      "Finished with run 17 out of 50 for era 1\n",
      "Finished with run 18 out of 50 for era 1\n",
      "Finished with run 19 out of 50 for era 1\n",
      "Finished with run 20 out of 50 for era 1\n",
      "Finished with run 21 out of 50 for era 1\n",
      "Finished with run 22 out of 50 for era 1\n",
      "Finished with run 23 out of 50 for era 1\n",
      "Finished with run 24 out of 50 for era 1\n",
      "Finished with run 25 out of 50 for era 1\n",
      "Finished with run 26 out of 50 for era 1\n",
      "Finished with run 27 out of 50 for era 1\n",
      "Finished with run 28 out of 50 for era 1\n",
      "Finished with run 29 out of 50 for era 1\n",
      "Finished with run 30 out of 50 for era 1\n",
      "Finished with run 31 out of 50 for era 1\n",
      "Finished with run 32 out of 50 for era 1\n",
      "Finished with run 33 out of 50 for era 1\n",
      "Finished with run 34 out of 50 for era 1\n",
      "Finished with run 35 out of 50 for era 1\n",
      "Finished with run 36 out of 50 for era 1\n",
      "Finished with run 37 out of 50 for era 1\n",
      "Finished with run 38 out of 50 for era 1\n",
      "Finished with run 39 out of 50 for era 1\n",
      "Finished with run 40 out of 50 for era 1\n",
      "Finished with run 41 out of 50 for era 1\n",
      "Finished with run 42 out of 50 for era 1\n",
      "Finished with run 43 out of 50 for era 1\n",
      "Finished with run 44 out of 50 for era 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bootstrap_eras(list_of_lists, n_bootstraps\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, vector_size\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, min_count\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m, sg\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, hs\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, negative\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, window\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m, workers\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[38], line 10\u001b[0m, in \u001b[0;36mbootstrap_eras\u001b[0;34m(list_of_lists, n_bootstraps, vector_size, min_count, epochs, sg, hs, negative, window, workers)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_bootstraps):\n\u001b[1;32m      9\u001b[0m     sentence_samples \u001b[39m=\u001b[39m resample(era)\n\u001b[0;32m---> 10\u001b[0m     model \u001b[39m=\u001b[39m Word2Vec(sentence_samples, vector_size\u001b[39m=\u001b[39;49mvector_size, min_count\u001b[39m=\u001b[39;49mmin_count, epochs\u001b[39m=\u001b[39;49mepochs, sg\u001b[39m=\u001b[39;49msg, hs\u001b[39m=\u001b[39;49mhs, negative\u001b[39m=\u001b[39;49mnegative, window\u001b[39m=\u001b[39;49mwindow, workers\u001b[39m=\u001b[39;49mworkers)\n\u001b[1;32m     11\u001b[0m     sim_stats_era\u001b[39m.\u001b[39mappend(model)\n\u001b[1;32m     12\u001b[0m     \u001b[39m# Save each bootstrapped model as a .pkl file\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/gensim/models/word2vec.py:430\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, passes\u001b[39m=\u001b[39m(epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[1;32m    429\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuild_vocab(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, trim_rule\u001b[39m=\u001b[39mtrim_rule)\n\u001b[0;32m--> 430\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m    431\u001b[0m         corpus_iterable\u001b[39m=\u001b[39;49mcorpus_iterable, corpus_file\u001b[39m=\u001b[39;49mcorpus_file, total_examples\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcorpus_count,\n\u001b[1;32m    432\u001b[0m         total_words\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcorpus_total_words, epochs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepochs, start_alpha\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malpha,\n\u001b[1;32m    433\u001b[0m         end_alpha\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmin_alpha, compute_loss\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss, callbacks\u001b[39m=\u001b[39;49mcallbacks)\n\u001b[1;32m    434\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     \u001b[39mif\u001b[39;00m trim_rule \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/gensim/models/word2vec.py:1073\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[0;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     callback\u001b[39m.\u001b[39mon_epoch_begin(\u001b[39mself\u001b[39m)\n\u001b[1;32m   1072\u001b[0m \u001b[39mif\u001b[39;00m corpus_iterable \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1073\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_epoch(\n\u001b[1;32m   1074\u001b[0m         corpus_iterable, cur_epoch\u001b[39m=\u001b[39;49mcur_epoch, total_examples\u001b[39m=\u001b[39;49mtotal_examples,\n\u001b[1;32m   1075\u001b[0m         total_words\u001b[39m=\u001b[39;49mtotal_words, queue_factor\u001b[39m=\u001b[39;49mqueue_factor, report_delay\u001b[39m=\u001b[39;49mreport_delay,\n\u001b[1;32m   1076\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1077\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1078\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_epoch_corpusfile(\n\u001b[1;32m   1079\u001b[0m         corpus_file, cur_epoch\u001b[39m=\u001b[39mcur_epoch, total_examples\u001b[39m=\u001b[39mtotal_examples, total_words\u001b[39m=\u001b[39mtotal_words,\n\u001b[1;32m   1080\u001b[0m         callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/gensim/models/word2vec.py:1434\u001b[0m, in \u001b[0;36mWord2Vec._train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[1;32m   1431\u001b[0m     thread\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m  \u001b[39m# make interrupting the process with ctrl+c easier\u001b[39;00m\n\u001b[1;32m   1432\u001b[0m     thread\u001b[39m.\u001b[39mstart()\n\u001b[0;32m-> 1434\u001b[0m trained_word_count, raw_word_count, job_tally \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_epoch_progress(\n\u001b[1;32m   1435\u001b[0m     progress_queue, job_queue, cur_epoch\u001b[39m=\u001b[39;49mcur_epoch, total_examples\u001b[39m=\u001b[39;49mtotal_examples,\n\u001b[1;32m   1436\u001b[0m     total_words\u001b[39m=\u001b[39;49mtotal_words, report_delay\u001b[39m=\u001b[39;49mreport_delay, is_corpus_file_mode\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1437\u001b[0m )\n\u001b[1;32m   1439\u001b[0m \u001b[39mreturn\u001b[39;00m trained_word_count, raw_word_count, job_tally\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/gensim/models/word2vec.py:1289\u001b[0m, in \u001b[0;36mWord2Vec._log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m   1286\u001b[0m unfinished_worker_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers\n\u001b[1;32m   1288\u001b[0m \u001b[39mwhile\u001b[39;00m unfinished_worker_count \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1289\u001b[0m     report \u001b[39m=\u001b[39m progress_queue\u001b[39m.\u001b[39;49mget()  \u001b[39m# blocks if workers too slow\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m     \u001b[39mif\u001b[39;00m report \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# a thread reporting that it finished\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m         unfinished_worker_count \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp/lib/python3.9/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait()\n\u001b[1;32m    172\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a non-negative number\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bootstrap_eras(list_of_lists, n_bootstraps=50, vector_size=100, min_count=0, epochs=200, sg=1, hs=0, negative=5, window=15, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(bootstrapped_eras, eurasianism, westphalianism, security_concerns, multipolarism):\n",
    "    similarity = []\n",
    "    for era in bootstrapped_eras:\n",
    "        sim_stats_eurasia = []\n",
    "        sim_stats_westph = []\n",
    "        sim_stats_sc = []\n",
    "        sim_stats_mp = []\n",
    "        for model in era:\n",
    "            for word, sim_stats in zip(eurasianism, sim_stats_eurasia):\n",
    "                try:\n",
    "                    sim_stats.append(model.wv.most_similar('миропорядок', word)[0][1])\n",
    "                except KeyError:\n",
    "                    sim_stats.append(np.nan)\n",
    "            for word, sim_stats in zip(westphalianism, sim_stats_westph):\n",
    "                try:\n",
    "                    sim_stats.append(model.wv.most_similar('миропорядок', word)[0][1])\n",
    "                except KeyError:\n",
    "                    sim_stats.append(np.nan)\n",
    "            for word, sim_stats in zip(security_concerns, sim_stats_sc):\n",
    "                try:\n",
    "                    sim_stats.append(model.wv.most_similar('миропорядок', word)[0][1])\n",
    "                except KeyError:\n",
    "                    sim_stats.append(np.nan)\n",
    "            for word, sim_stats in zip(multipolarism, sim_stats_mp):\n",
    "                try:\n",
    "                    sim_stats.append(model.wv.most_similar('миропорядок', word)[0][1])\n",
    "                except KeyError:\n",
    "                    sim_stats.append(np.nan)\n",
    "        similarity.append([sim_stats_eurasia, sim_stats_westph, sim_stats_sc, sim_stats_mp])\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving model output\n",
    "\n",
    "with open('naive_model_output.pkl', 'wb') as f:\n",
    "    pickle.dump(stat_types, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then run again the loop but this time just for the interested word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the cosine similarity between the matrix of entire corpus and the matrix of the single word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the mean of the bootstrap sample for vectors of sigle word and for vectors of all words from the corpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
