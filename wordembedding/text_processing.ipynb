{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.ru.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 36658 entries, 0 to 37587\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   date        36658 non-null  object\n",
      " 1   url         36658 non-null  object\n",
      " 2   title       36658 non-null  object\n",
      " 3   speaker     36658 non-null  object\n",
      " 4   text        36658 non-null  object\n",
      " 5   text_clean  36658 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "somi = pd.read_pickle('/Users/adrianacuppuleri/Desktop/GITHUB ADRIANA/Illiberal_discourse/data/corpus_adriana/corpus_president_of_russia/statements_on_major_issues.pkl')\n",
    "somi.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14 entries, 0 to 13\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   date        14 non-null     object\n",
      " 1   URL         14 non-null     object\n",
      " 2   title       14 non-null     object\n",
      " 3   speaker     14 non-null     object\n",
      " 4   text        14 non-null     object\n",
      " 5   text_clean  14 non-null     object\n",
      "dtypes: object(6)\n",
      "memory usage: 800.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "mtfa = pd.read_pickle('/Users/adrianacuppuleri/Desktop/GITHUB ADRIANA/Illiberal_discourse/data/corpus_adriana/corpus_president_of_russia/messages_to_federal_assembly.pkl')\n",
    "mtfa.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9642 entries, 0 to 9641\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   date        9642 non-null   object\n",
      " 1   url         9642 non-null   object\n",
      " 2   title       9642 non-null   object\n",
      " 3   speaker     9642 non-null   object\n",
      " 4   text        9642 non-null   object\n",
      " 5   text_clean  9642 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 452.1+ KB\n"
     ]
    }
   ],
   "source": [
    "interviews = pd.read_pickle('/Users/adrianacuppuleri/Desktop/GITHUB ADRIANA/Illiberal_discourse/data/corpus_adriana/corpus_president_of_russia/interviews.pkl')\n",
    "interviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33189 entries, 0 to 33188\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   date          33189 non-null  object\n",
      " 1   URL           33189 non-null  object\n",
      " 2   description   30316 non-null  object\n",
      " 3   introduction  30316 non-null  object\n",
      " 4   id            33189 non-null  int64 \n",
      " 5   speaker       33189 non-null  object\n",
      " 6   p             33189 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "news = pd.read_excel('data/putin_corpus.xlsx')\n",
    "news.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5208 entries, 0 to 5212\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   speaker     5208 non-null   object        \n",
      " 1   date        5208 non-null   datetime64[ns]\n",
      " 2   url         5208 non-null   object        \n",
      " 3   title       5208 non-null   object        \n",
      " 4   text        5208 non-null   object        \n",
      " 5   word_count  5208 non-null   int64         \n",
      " 6   lemma       5208 non-null   object        \n",
      "dtypes: datetime64[ns](1), int64(1), object(5)\n",
      "memory usage: 325.5+ KB\n"
     ]
    }
   ],
   "source": [
    "lavrov = pd.read_pickle('/Users/adrianacuppuleri/Desktop/GITHUB ADRIANA/Illiberal_discourse/data/corpus_adriana/corpus_lavrov/lavrov_clean.pkl')\n",
    "lavrov.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lavrov.rename(columns={'text':'text_clean'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "somi_url : 533\n",
      "mtfa_url : 14\n",
      "interviews_url : 114\n",
      "news_url : 871\n",
      "lavrov_url : 5208\n"
     ]
    }
   ],
   "source": [
    "#count the urls for each dataset\n",
    "print(f\"somi_url : {len(somi.url.value_counts())}\")\n",
    "print(f\"mtfa_url : {len(mtfa.URL.value_counts())}\")\n",
    "print(f\"interviews_url : {len(interviews.url.value_counts())}\")\n",
    "print(f\"news_url : {len(news.URL.value_counts())}\")\n",
    "print(f\"lavrov_url : {len(lavrov.url.value_counts())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    # replace actual newline characters \\n with a space\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    #replace Д.Медведев:|В.Путин: with a white space\n",
    "    text = re.sub(r'(Д\\.Медведев:|В\\.Путин:)', ' ', text)\n",
    "    # replace everything non-alphanumeric with a space\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    # replace two or more dots with one\n",
    "    text = re.sub(r'\\.{2,}', ' ', text)\n",
    "    # replace sequences of white spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # replace \\xa0 with a white space\n",
    "    text = re.sub(r'\\xa0', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "news['text_clean'] = news['p'].map(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33189 entries, 0 to 33188\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   date          33189 non-null  object\n",
      " 1   URL           33189 non-null  object\n",
      " 2   description   30316 non-null  object\n",
      " 3   introduction  30316 non-null  object\n",
      " 4   id            33189 non-null  int64 \n",
      " 5   speaker       33189 non-null  object\n",
      " 6   p             33189 non-null  object\n",
      " 7   text_clean    33189 non-null  object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "news.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each dataset, add a col word_count\n",
    "#combine the dfs in a list\n",
    "dfs = [somi, mtfa, interviews, news,lavrov]\n",
    "# iterate over each dataframe and extract the word count for the 'text_clean' column and cut row where word_count == 0\n",
    "for df in dfs:\n",
    "    df['word_count'] = df['text_clean'].apply(lambda x: len(x.split()))\n",
    "    df.drop(df[df['word_count'] == 0].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 rows in 36547 have word_count greater than 250.\n",
      "14 rows in 14 have word_count greater than 250.\n",
      "9 rows in 9380 have word_count greater than 250.\n",
      "4 rows in 33188 have word_count greater than 250.\n",
      "3808 rows in 5207 have word_count greater than 250.\n"
     ]
    }
   ],
   "source": [
    "#how many rows are >250 words\n",
    "for df in dfs:\n",
    "    count = df[df['word_count'] > 250]['word_count'].count()\n",
    "    print(f\"{count} rows in {len(df)} have word_count greater than 250.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split the text_clean into paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(text, n):\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i+n]) for i in range(0, len(words), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to the 'text' column of the mtfa\n",
    "chunk_size = 250\n",
    "mtfa['chunks'] = mtfa['text_clean'].apply(lambda x: split_into_chunks(x, chunk_size))\n",
    "# unnest the chunks column to create a new row for each chunk\n",
    "new_mtfa = mtfa.explode('chunks')\n",
    "# reset the index of the dataframe\n",
    "new_mtfa = new_mtfa.reset_index(drop=True)\n",
    "new_mtfa['word_count_chunk'] = new_mtfa['chunks'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to the 'text' column of the lavrov\n",
    "chunk_size = 250\n",
    "lavrov['chunks'] = lavrov['text_clean'].apply(lambda x: split_into_chunks(x, chunk_size))\n",
    "# unnest the chunks column to create a new row for each chunk\n",
    "new_lavrov = lavrov.explode('chunks')\n",
    "# reset the index of the dataframe\n",
    "new_lavrov = new_lavrov.reset_index(drop=True)\n",
    "new_lavrov['word_count_chunk'] = new_lavrov['chunks'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to the 'text' column of the somi\n",
    "chunk_size = 250\n",
    "somi['chunks'] = somi['text_clean'].apply(lambda x: split_into_chunks(x, chunk_size))\n",
    "# unnest the chunks column to create a new row for each chunk\n",
    "new_somi = somi.explode('chunks')\n",
    "# reset the index of the dataframe\n",
    "new_somi = new_somi.reset_index(drop=True)\n",
    "new_somi['word_count_chunk'] = new_somi['chunks'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to the 'text' column of the interviews\n",
    "chunk_size = 250\n",
    "interviews['chunks'] = interviews['text_clean'].apply(lambda x: split_into_chunks(x, chunk_size))\n",
    "# unnest the chunks column to create a new row for each chunk\n",
    "new_interviews = interviews.explode('chunks')\n",
    "# reset the index of the dataframe\n",
    "new_interviews = new_interviews.reset_index(drop=True)\n",
    "new_interviews['word_count_chunk'] = new_interviews['chunks'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to the 'text' column of the news\n",
    "chunk_size = 250\n",
    "news['chunks'] = news['text_clean'].apply(lambda x: split_into_chunks(x, chunk_size))\n",
    "# unnest the chunks column to create a new row for each chunk\n",
    "new_news = news.explode('chunks')\n",
    "# reset the index of the dataframe\n",
    "new_news= new_news.reset_index(drop=True)\n",
    "new_news['word_count_chunk'] = new_news['chunks'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rows in 9389 have word_count greater than 250.\n",
      "0 rows in 24947 have word_count greater than 250.\n",
      "0 rows in 488 have word_count greater than 250.\n",
      "0 rows in 36565 have word_count greater than 250.\n",
      "0 rows in 33192 have word_count greater than 250.\n"
     ]
    }
   ],
   "source": [
    "dfs = [new_interviews, new_lavrov, new_mtfa, new_somi, new_news]\n",
    "for df in dfs:\n",
    "    count = df[df['word_count_chunk'] > 250]['word_count_chunk'].count()\n",
    "    print(f\"{count} rows in {len(df)} have word_count greater than 250.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop lemma col on lavrov\n",
    "new_lavrov = new_lavrov.drop('lemma', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokens and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_spacy = spacy.load('ru_core_news_sm')\n",
    "nlp_spacy.disable_pipe(\"parser\")\n",
    "nlp_spacy.enable_pipe(\"senter\")\n",
    "nlp_spacy.enable_pipe(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopwords\n",
    "from spacy.lang.ru import stop_words\n",
    "nlp_spacy.Defaults.stop_words |= { 'два','день','дорогой','добрый','коллега','раз','сегодня','спасибо','уважаемый','уважаемые'}\n",
    "stop_words = stop_words.STOP_WORDS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token, POS, Lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs(docs):\n",
    "    for doc in nlp_spacy.pipe(docs, batch_size=50, n_process=-1):\n",
    "        tokens = [token.text.lower() for token in doc]\n",
    "        lemma = [token.lemma_.lower() for token in doc if token.lemma_.lower() not in stop_words]\n",
    "        pos = [token.pos_ for token in doc]\n",
    "        yield tokens, lemma, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mtfa['tokens'], new_mtfa['lemma'], new_mtfa['pos'] = zip(*process_docs(new_mtfa['chunks'].astype('unicode').values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_somi['tokens'], new_somi['lemma'], new_somi['pos'] = zip(*process_docs(new_somi['chunks'].astype('unicode').values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_interviews['tokens'], new_interviews['lemma'], new_interviews['pos'] = zip(*process_docs(new_interviews['chunks'].astype('unicode').values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_news['tokens'], new_news['lemma'], new_news['pos'] = zip(*process_docs(new_news['chunks'].astype('unicode').values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lavrov['tokens'], new_lavrov['lemma'], new_lavrov['pos'] = zip(*process_docs(new_lavrov['chunks'].astype('unicode').values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[я, наверное, не, буду, выступать, с, речью, м...</td>\n",
       "      <td>[наверное, выступать, речь, продуктивный, побе...</td>\n",
       "      <td>[PRON, ADV, PART, AUX, VERB, ADP, NOUN, PRON, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[обсуждали, в, числе, прочего, ситуацию, в, ли...</td>\n",
       "      <td>[обсуждать, число, ситуация, ливия, разуметься...</td>\n",
       "      <td>[VERB, ADP, NOUN, ADJ, NOUN, ADP, PROPN, CCONJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[сказать, по, поводу, игил, наиболее, боеспосо...</td>\n",
       "      <td>[сказать, повод, игил, боеспособный, часть, иг...</td>\n",
       "      <td>[VERB, ADP, NOUN, PROPN, ADV, ADJ, NOUN, PROPN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[в, лавров, этим, давно, уже, пора, было, заня...</td>\n",
       "      <td>[лавр, заняться, буквально, состояться, коротк...</td>\n",
       "      <td>[ADP, NOUN, PRON, ADV, ADV, NOUN, AUX, VERB, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[коалицией, в, нее, вошли, страны, которые, ре...</td>\n",
       "      <td>[коалиция, войти, страна, решить, поддерживать...</td>\n",
       "      <td>[NOUN, ADP, PRON, VERB, NOUN, PRON, VERB, VERB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [я, наверное, не, буду, выступать, с, речью, м...   \n",
       "1  [обсуждали, в, числе, прочего, ситуацию, в, ли...   \n",
       "2  [сказать, по, поводу, игил, наиболее, боеспосо...   \n",
       "3  [в, лавров, этим, давно, уже, пора, было, заня...   \n",
       "4  [коалицией, в, нее, вошли, страны, которые, ре...   \n",
       "\n",
       "                                               lemma  \\\n",
       "0  [наверное, выступать, речь, продуктивный, побе...   \n",
       "1  [обсуждать, число, ситуация, ливия, разуметься...   \n",
       "2  [сказать, повод, игил, боеспособный, часть, иг...   \n",
       "3  [лавр, заняться, буквально, состояться, коротк...   \n",
       "4  [коалиция, войти, страна, решить, поддерживать...   \n",
       "\n",
       "                                                 pos  \n",
       "0  [PRON, ADV, PART, AUX, VERB, ADP, NOUN, PRON, ...  \n",
       "1  [VERB, ADP, NOUN, ADJ, NOUN, ADP, PROPN, CCONJ...  \n",
       "2  [VERB, ADP, NOUN, PROPN, ADV, ADJ, NOUN, PROPN...  \n",
       "3  [ADP, NOUN, PRON, ADV, ADV, NOUN, AUX, VERB, A...  \n",
       "4  [NOUN, ADP, PRON, VERB, NOUN, PRON, VERB, VERB...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lavrov[['tokens','lemma','pos']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [new_interviews, new_lavrov, new_mtfa, new_news, new_somi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_interviews.to_pickle('/Users/adrianacuppuleri/Desktop/GITHUB ADRIANA/Illiberal_discourse/data/corpus_adriana/token_lemma_pos/new_interviews.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/Users/adrianacuppuleri/Desktop/GITHUB ADRIANA/Illiberal_discourse/data/corpus_adriana/token_lemma_pos'\n",
    "names = ['new_interviews', 'new_lavrov', 'new_mtfa', 'new_news', 'new_somi']\n",
    "\n",
    "for name, df in zip(names, dfs):\n",
    "    # construct the output file path for the pickle file\n",
    "    output_path = os.path.join(output_dir, f\"{name}.pkl\")\n",
    "    # save the DataFrame to a pickle file\n",
    "    df.to_pickle(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>word_count</th>\n",
       "      <th>chunks</th>\n",
       "      <th>word_count_chunk</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-03-06</td>\n",
       "      <td>http://kremlin.ru/events/president/transcripts...</td>\n",
       "      <td>Интервью российским и иностранным СМИ в преддв...</td>\n",
       "      <td>Вопрос:</td>\n",
       "      <td>С точки зрения внимания общественности и СМИ, ...</td>\n",
       "      <td>с точки зрения внимания общественности и сми л...</td>\n",
       "      <td>30</td>\n",
       "      <td>с точки зрения внимания общественности и сми л...</td>\n",
       "      <td>30</td>\n",
       "      <td>[с, точки, зрения, внимания, общественности, и...</td>\n",
       "      <td>[точка, зрение, внимание, общественность, сми,...</td>\n",
       "      <td>[ADP, NOUN, NOUN, NOUN, NOUN, CCONJ, NOUN, ADJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-03-06</td>\n",
       "      <td>http://kremlin.ru/events/president/transcripts...</td>\n",
       "      <td>Интервью российским и иностранным СМИ в преддв...</td>\n",
       "      <td>В.Путин:</td>\n",
       "      <td>Начнём с того, что Великобританию принято счит...</td>\n",
       "      <td>начнём с того что великобританию принято счита...</td>\n",
       "      <td>81</td>\n",
       "      <td>начнём с того что великобританию принято счита...</td>\n",
       "      <td>81</td>\n",
       "      <td>[начнём, с, того, что, великобританию, принято...</td>\n",
       "      <td>[начнём, великобританию, принять, считать, род...</td>\n",
       "      <td>[VERB, ADP, PRON, SCONJ, NOUN, VERB, VERB, NOU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-03-06</td>\n",
       "      <td>http://kremlin.ru/events/president/transcripts...</td>\n",
       "      <td>Интервью российским и иностранным СМИ в преддв...</td>\n",
       "      <td>В.Путин:</td>\n",
       "      <td>Наша страна принимает участие в Паралимпиадах ...</td>\n",
       "      <td>наша страна принимает участие в паралимпиадах ...</td>\n",
       "      <td>30</td>\n",
       "      <td>наша страна принимает участие в паралимпиадах ...</td>\n",
       "      <td>30</td>\n",
       "      <td>[наша, страна, принимает, участие, в, паралимп...</td>\n",
       "      <td>[страна, принимать, участие, паралимпиадах, во...</td>\n",
       "      <td>[DET, NOUN, VERB, NOUN, ADP, NOUN, ADP, ADJ, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-03-06</td>\n",
       "      <td>http://kremlin.ru/events/president/transcripts...</td>\n",
       "      <td>Интервью российским и иностранным СМИ в преддв...</td>\n",
       "      <td>В.Путин:</td>\n",
       "      <td>В целом паралимпийский спорт, его замечательны...</td>\n",
       "      <td>в целом паралимпийский спорт его замечательные...</td>\n",
       "      <td>40</td>\n",
       "      <td>в целом паралимпийский спорт его замечательные...</td>\n",
       "      <td>40</td>\n",
       "      <td>[в, целом, паралимпийский, спорт, его, замечат...</td>\n",
       "      <td>[целое, паралимпийский, спорт, замечательный, ...</td>\n",
       "      <td>[ADP, NOUN, ADJ, NOUN, DET, ADJ, NOUN, VERB, P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-03-06</td>\n",
       "      <td>http://kremlin.ru/events/president/transcripts...</td>\n",
       "      <td>Интервью российским и иностранным СМИ в преддв...</td>\n",
       "      <td>В.Путин:</td>\n",
       "      <td>Кроме того, мы будем масштабно транслировать э...</td>\n",
       "      <td>кроме того мы будем масштабно транслировать эт...</td>\n",
       "      <td>43</td>\n",
       "      <td>кроме того мы будем масштабно транслировать эт...</td>\n",
       "      <td>43</td>\n",
       "      <td>[кроме, того, мы, будем, масштабно, транслиров...</td>\n",
       "      <td>[масштабно, транслировать, соревнование, между...</td>\n",
       "      <td>[ADP, PRON, PRON, AUX, ADV, VERB, DET, NOUN, A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                                url  \\\n",
       "0  2014-03-06  http://kremlin.ru/events/president/transcripts...   \n",
       "1  2014-03-06  http://kremlin.ru/events/president/transcripts...   \n",
       "2  2014-03-06  http://kremlin.ru/events/president/transcripts...   \n",
       "3  2014-03-06  http://kremlin.ru/events/president/transcripts...   \n",
       "4  2014-03-06  http://kremlin.ru/events/president/transcripts...   \n",
       "\n",
       "                                               title   speaker  \\\n",
       "0  Интервью российским и иностранным СМИ в преддв...   Вопрос:   \n",
       "1  Интервью российским и иностранным СМИ в преддв...  В.Путин:   \n",
       "2  Интервью российским и иностранным СМИ в преддв...  В.Путин:   \n",
       "3  Интервью российским и иностранным СМИ в преддв...  В.Путин:   \n",
       "4  Интервью российским и иностранным СМИ в преддв...  В.Путин:   \n",
       "\n",
       "                                                text  \\\n",
       "0  С точки зрения внимания общественности и СМИ, ...   \n",
       "1  Начнём с того, что Великобританию принято счит...   \n",
       "2  Наша страна принимает участие в Паралимпиадах ...   \n",
       "3  В целом паралимпийский спорт, его замечательны...   \n",
       "4  Кроме того, мы будем масштабно транслировать э...   \n",
       "\n",
       "                                          text_clean  word_count  \\\n",
       "0  с точки зрения внимания общественности и сми л...          30   \n",
       "1  начнём с того что великобританию принято счита...          81   \n",
       "2  наша страна принимает участие в паралимпиадах ...          30   \n",
       "3  в целом паралимпийский спорт его замечательные...          40   \n",
       "4  кроме того мы будем масштабно транслировать эт...          43   \n",
       "\n",
       "                                              chunks  word_count_chunk  \\\n",
       "0  с точки зрения внимания общественности и сми л...                30   \n",
       "1  начнём с того что великобританию принято счита...                81   \n",
       "2  наша страна принимает участие в паралимпиадах ...                30   \n",
       "3  в целом паралимпийский спорт его замечательные...                40   \n",
       "4  кроме того мы будем масштабно транслировать эт...                43   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [с, точки, зрения, внимания, общественности, и...   \n",
       "1  [начнём, с, того, что, великобританию, принято...   \n",
       "2  [наша, страна, принимает, участие, в, паралимп...   \n",
       "3  [в, целом, паралимпийский, спорт, его, замечат...   \n",
       "4  [кроме, того, мы, будем, масштабно, транслиров...   \n",
       "\n",
       "                                               lemma  \\\n",
       "0  [точка, зрение, внимание, общественность, сми,...   \n",
       "1  [начнём, великобританию, принять, считать, род...   \n",
       "2  [страна, принимать, участие, паралимпиадах, во...   \n",
       "3  [целое, паралимпийский, спорт, замечательный, ...   \n",
       "4  [масштабно, транслировать, соревнование, между...   \n",
       "\n",
       "                                                 pos  \n",
       "0  [ADP, NOUN, NOUN, NOUN, NOUN, CCONJ, NOUN, ADJ...  \n",
       "1  [VERB, ADP, PRON, SCONJ, NOUN, VERB, VERB, NOU...  \n",
       "2  [DET, NOUN, VERB, NOUN, ADP, NOUN, ADP, ADJ, N...  \n",
       "3  [ADP, NOUN, ADJ, NOUN, DET, ADJ, NOUN, VERB, P...  \n",
       "4  [ADP, PRON, PRON, AUX, ADV, VERB, DET, NOUN, A...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_interviews = pd.read_pickle('data/corpus_adriana/token_lemma_pos/new_interviews.pkl')\n",
    "new_interviews.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
